# -*- coding: utf-8 -*-
"""Auto Car Insurance fraud claim Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TFT5JqT1bClO1z_HyJ00TaNGWli7r0D3
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

#Saving The Model
import joblib
import pickle


import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler


from sklearn.metrics import accuracy_score

from sklearn.model_selection import train_test_split



from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier


#Hyper parameter tuning
from sklearn.model_selection import GridSearchCV

data = pd.read_csv("./insurance fraud claims.csv")

data.head()

data.describe()

data.info()

"""### CHECK FOR MISSING VALUES"""

data.isnull().sum()

data.drop("_c39", axis=1, inplace=True)

data

for column in data.columns:
  data.groupby(column).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
  plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt

for column in data.columns:
  data.plot(kind='scatter', x=column, y='fraud_reported', s=32, alpha=.8)
  plt.gca().spines[['top', 'right',]].set_visible(False)

label_encoder = LabelEncoder()


data["fraud_reported"] = label_encoder.fit_transform(data["fraud_reported"])

data.corr(method="spearman")

corr_data = data.corr(method="spearman")



plt.figure(figsize=(12, 10))

sns.heatmap(corr_data, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Heatmap')

# Show plot
plt.show()

"""**TO FIND UNIQUE VALUES OF OBJECT VALUES**"""

classes_with_too_many_instances = []


for i in data.columns:
  if data[i].dtype == 'object':
    if data[i].nunique() > 10:
      classes_with_too_many_instances.append(i)

    print(i,":", data[i].nunique())


print(classes_with_too_many_instances)

for column in data.columns:
    # Check if '?' exists in the column
    if data[column].dtype == "object":

      if data[column].astype(str).str.contains('\?').any():
        print(f"'?' exists in column: {column}")

data.replace('?', np.nan, inplace=True)

"""Replace Missing Object (String) values with the most occurence"""

# prompt: replace nan with the mode of the each columns in th data dataframe

for column in data.columns:
  if data[column].dtype == "object":
    data[column].fillna(data[column].mode()[0], inplace=True)

data.info()

# Compute correlation coefficients between numeric features and target variable
numeric_features = data.select_dtypes(include=['int64', 'float64']).columns
correlation_with_fraud = data[numeric_features].corrwith(data['fraud_reported'])

# Sort the correlations in descending order
sorted_correlation = correlation_with_fraud.abs().sort_values(ascending=False)

print("Correlation coefficients with 'fraud_reported':")
print(sorted_correlation)

"""### DROP NOT NEEDED COLUMNS FOR MODEL TRAINING

"""

data = data.drop([
  'policy_bind_date',
  "policy_number",
  "policy_csl",
  'insured_occupation',
  'insured_hobbies',
  'incident_date',
  'incident_location',
  'auto_make',
  "policy_number",
  "insured_zip",
  "capital-gains",
  "capital-loss" ,
  "policy_annual_premium",
  "auto_year",
  "incident_hour_of_the_day",
  'auto_model',
  "incident_date",
  "incident_state",
  "incident_city" ,
  "incident_location",
  "incident_hour_of_the_day" ],
    axis=1)

data.info()

data = data.drop([
  'insured_sex', 'insured_relationship', 'insured_education_level'],
    axis=1)

data.info()

data["policy_state"].describe()

policy_state_one_hot_encoded = pd.get_dummies(data['policy_state'], prefix='policy_state_encoded')

data = pd.concat([data, policy_state_one_hot_encoded], axis=1)

data.drop('policy_state', axis=1, inplace=True)

data

data["incident_type"].describe()

label_encoder = LabelEncoder()

data['incident_type'] = label_encoder.fit_transform(data['incident_type'])

label_encoder = LabelEncoder()

data['authorities_contacted'] = label_encoder.fit_transform(data['authorities_contacted'])

label_encoder = LabelEncoder()

data['collision_type'] = label_encoder.fit_transform(data['collision_type'])

label_encoder = LabelEncoder()

data['incident_severity'] = label_encoder.fit_transform(data['incident_severity'])

label_encoder = LabelEncoder()

data['property_damage'] = label_encoder.fit_transform(data['property_damage'])

data.info()

label_encoder = LabelEncoder()

data['police_report_available'] = label_encoder.fit_transform(data['police_report_available'])

"""### SEPERARING TO X AND Y"""

data

X = data.drop(["fraud_reported", "property_claim", "injury_claim", "vehicle_claim"], axis=1)

Y = data["fraud_reported"]

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)

scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)

x_trained = pd.DataFrame(x_train_scaled, columns=x_train.columns)

x_test_scaled = scaler.fit_transform(x_test)

x_tested = pd.DataFrame(x_test_scaled, columns=x_test.columns)

"""### Training of Model

#### SVC MODEL
"""

svc_model = SVC()


svc_model.fit(x_trained, y_train)

y_pred = svc_model.predict(x_tested)


svc_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of SVC for testing data is :","{:.2f}%".format(100*svc_accuracy))

# from sklearn.svm import SVC
# from sklearn.linear_model import LogisticRegression
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.naive_bayes import GaussianNB
# from sklearn.neural_network import MLPClassifier

log_model = LogisticRegression()

log_model.fit(x_trained, y_train)

y_pred = log_model.predict(x_tested)


log_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of LogisticRegression for testing data is :","{:.2f}%".format(100*log_accuracy))

random_forest_model = RandomForestClassifier()

random_forest_model.fit(x_trained, y_train)

y_pred = random_forest_model.predict(x_tested)

random_forest_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of RandomForestClassifier for testing data is :","{:.2f}%".format(100*random_forest_accuracy))

gradient_boost_model = GradientBoostingClassifier()

gradient_boost_model.fit(x_trained, y_train)

y_pred = gradient_boost_model.predict(x_tested)

gradient_boost_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of GradientBoostingClassifier for testing data is :","{:.2f}%".format(100*gradient_boost_accuracy))

knn_model = KNeighborsClassifier()

knn_model.fit(x_trained, y_train)

y_pred = knn_model.predict(x_tested)

knn_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of KNeighborsClassifier for testing data is :","{:.2f}%".format(100*knn_accuracy))

decision_tree_model = DecisionTreeClassifier()

decision_tree_model.fit(x_trained, y_train)

y_pred = decision_tree_model.predict(x_tested)

decision_tree_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of DecisionTreeClassifier for testing data is :","{:.2f}%".format(100*decision_tree_accuracy))

gaussian_model = GaussianNB()

gaussian_model.fit(x_trained, y_train)

y_pred = gaussian_model.predict(x_tested)

gaussian_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of GaussianNB for testing data is :","{:.2f}%".format(100*gaussian_accuracy))

mlp_model = MLPClassifier()

mlp_model.fit(x_trained, y_train)

y_pred = mlp_model.predict(x_tested)

mlp_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of MLPClassifier for testing data is :","{:.2f}%".format(100*mlp_accuracy))

from xgboost import XGBClassifier

xgb_model = XGBClassifier()

xgb_model.fit(x_trained, y_train)

y_pred = xgb_model.predict(x_tested)

xgb_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of XGBClassifier for testing data is :","{:.2f}%".format(100*xgb_accuracy))

knn_model = KNeighborsClassifier()

knn_model.fit(x_trained, y_train)

y_pred = knn_model.predict(x_tested)

knn_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of knnClassifier for testing data is :","{:.2f}%".format(100*knn_accuracy))

# Using Neural Network

import tensorflow as tf
from tensorflow import keras




model = keras.Sequential()
model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(x_trained.shape[1],)))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))



model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


model.fit(x_trained, y_train, epochs=12, batch_size=32, validation_split=0.1)

# Neural Network Accuracy
loss, neaural_network_accuracy = model.evaluate(x_tested, y_test)


print("Accuracy of Neaural Network for testing data is :","{:.2f}%".format(100*neaural_network_accuracy))

models = pd.DataFrame({
    'Model' : ['LogisticRegression','SVM', 'KNN', 'Decision Tree', 'Random Forest', 'Gaussian', 'XGBoost', "Neural Network", "MLP"],
    'Score' : [log_accuracy,svc_accuracy,knn_accuracy, decision_tree_accuracy, random_forest_accuracy, gaussian_accuracy,xgb_accuracy, neaural_network_accuracy, mlp_accuracy]
})

models.sort_values(by = 'Score', ascending = False)

fig,ax = plt.subplots(figsize=(10,10))
barplot =sns.barplot(data=models,x='Score', y='Model',palette='bright',order=models.sort_values('Score',ascending=False).Model,ax=ax)
ax.set(xlim=(0,1))
for p in ax.patches:
    ax.annotate("%.2f" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()/2),
            xytext=(5, 0), textcoords='offset points', ha="left", va="center")

"""## Saving the Mdoel

### HYPER PARAMETER TUNING

LOGISTIC REGRESSION
"""

param_grid_logistic = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse regularization strength
    'solver': ['liblinear', 'lbfgs']  # Solvers for Logistic Regression
}

param_grid_svc = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']  # Different kernel functions for SVM
}

param_grid_random_forest = {
    'n_estimators': [100, 200, 500],  # Number of trees in the forest
    'max_depth': [4, 8, 16]  # Maximum depth of individual trees
}

param_grid_knn = {
    'n_neighbors': [3, 5, 10],  # Number of neighbors to consider
}

param_grid_xgboost = {
    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate for boosting
    'n_estimators': [100, 200, 500]  # Number of boosting stages
}

param_grid_mlp = {
    'hidden_layer_sizes': [(100,), (100, 50), (100, 100)],  # Number of neurons in hidden layers
    'activation': ['relu', 'tanh']  # Activation function for neurons
}

# Create and run GridSearchCV for each model
models = []
for model_name, model,  param_grid in [
    ('Logistic Regression', LogisticRegression(), param_grid_logistic),

    ('Gaussian Naive Bayes', GaussianNB(), None),  # No hyperparameters to tune for Naive Bayes

    ('SVC', SVC() ,  param_grid_svc),

    ('Random Forest', RandomForestClassifier(), param_grid_random_forest),

    ('KNN',KNeighborsClassifier(), param_grid_knn),

    ('XGBoost', XGBClassifier() , param_grid_xgboost),

    ('MLP', MLPClassifier(), param_grid_mlp)
]:
    if model_name != 'Gaussian Naive Bayes':  # Naive Bayes already instantiated
        model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
        model.fit(x_trained, y_train)
    else:
        model = model.fit(x_trained, y_train)  # Naive Bayes fit directly
    models.append((model_name, model))

best_accuracy = 0
best_model = None
for model_name, model in models:
  if not isinstance(model, GaussianNB):

    y_pred =  model.best_estimator_.predict(x_tested)

    model_accuracy = accuracy_score(y_test, y_pred)

    if model_accuracy > best_accuracy:
            best_accuracy = model_accuracy
            best_model = model.best_estimator_


    print(f"\n Model acccuracy for {model_name}:", model_accuracy)

joblib.dump(best_model, 'insurace_fraud_model_SVC_Ignaz.pkl')



